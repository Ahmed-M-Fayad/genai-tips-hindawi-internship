{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lab 1 (YouTube Videos Summarizer)","metadata":{}},{"cell_type":"markdown","source":"A Demo for summarization app utilizing youtube transcript API and Bart summary generator model\n\nMethodology:\n1. Extracting the video id from a provided input link\n2. Transcript is extracted using pre-built function]\n3. Providing Bart with the transcript for summarization","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Installing Required Packages","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.52.4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T17:58:57.617098Z","iopub.execute_input":"2025-10-21T17:58:57.617698Z","iopub.status.idle":"2025-10-21T17:59:15.063396Z","shell.execute_reply.started":"2025-10-21T17:58:57.617672Z","shell.execute_reply":"2025-10-21T17:59:15.062448Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.52.4\n  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.30.0 (from transformers==4.52.4)\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.4) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.52.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.52.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.52.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.52.4) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.52.4) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.52.4) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.4) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.52.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.52.4) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.52.4) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.52.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.52.4) (2024.2.0)\nDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.35.3 transformers-4.52.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install youtube-transcript-api","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T17:59:15.065110Z","iopub.execute_input":"2025-10-21T17:59:15.065456Z","iopub.status.idle":"2025-10-21T17:59:18.611049Z","shell.execute_reply.started":"2025-10-21T17:59:15.065433Z","shell.execute_reply":"2025-10-21T17:59:18.610275Z"}},"outputs":[{"name":"stdout","text":"Collecting youtube-transcript-api\n  Downloading youtube_transcript_api-1.2.3-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (2.32.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2025.8.3)\nDownloading youtube_transcript_api-1.2.3-py3-none-any.whl (485 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.1/485.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: youtube-transcript-api\nSuccessfully installed youtube-transcript-api-1.2.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## HuggingFace Connection","metadata":{}},{"cell_type":"markdown","source":"For Retrieving Models","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T17:59:18.611966Z","iopub.execute_input":"2025-10-21T17:59:18.612248Z","iopub.status.idle":"2025-10-21T17:59:18.937342Z","shell.execute_reply.started":"2025-10-21T17:59:18.612214Z","shell.execute_reply":"2025-10-21T17:59:18.936464Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"424ae4a4bd394023bcb7013b11b5102a"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Defining Extraction Function","metadata":{}},{"cell_type":"code","source":"from urllib.parse import urlparse, parse_qs\nfrom youtube_transcript_api import YouTubeTranscriptApi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T17:59:25.524023Z","iopub.execute_input":"2025-10-21T17:59:25.524694Z","iopub.status.idle":"2025-10-21T17:59:25.532267Z","shell.execute_reply.started":"2025-10-21T17:59:25.524671Z","shell.execute_reply":"2025-10-21T17:59:25.531743Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def extract_video_id(url: str) -> str:\n    \"\"\"\n    Extract the YouTube video ID from a URL.\n    Raises ValueError if no 'v' parameter is found.\n    \"\"\"\n    parsed = urlparse(url)\n    qs = parse_qs(parsed.query)\n    video_ids = qs.get('v')\n    \n    if not video_ids:\n        raise ValueError(f\"No video id found in URL: {url}\")\n    return video_ids[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T17:59:26.913938Z","iopub.execute_input":"2025-10-21T17:59:26.914453Z","iopub.status.idle":"2025-10-21T17:59:26.918752Z","shell.execute_reply.started":"2025-10-21T17:59:26.914433Z","shell.execute_reply":"2025-10-21T17:59:26.918143Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Calling Bart (Summariztion Model)","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, set_seed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T17:59:28.792034Z","iopub.execute_input":"2025-10-21T17:59:28.792687Z","iopub.status.idle":"2025-10-21T18:00:02.850947Z","shell.execute_reply.started":"2025-10-21T17:59:28.792657Z","shell.execute_reply":"2025-10-21T18:00:02.850281Z"}},"outputs":[{"name":"stderr","text":"2025-10-21 17:59:42.706955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761069583.100397      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761069583.220459      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# setting randomization\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:02.852124Z","iopub.execute_input":"2025-10-21T18:00:02.852589Z","iopub.status.idle":"2025-10-21T18:00:02.865202Z","shell.execute_reply.started":"2025-10-21T18:00:02.852569Z","shell.execute_reply":"2025-10-21T18:00:02.864430Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:02.866038Z","iopub.execute_input":"2025-10-21T18:00:02.866319Z","iopub.status.idle":"2025-10-21T18:00:11.993548Z","shell.execute_reply.started":"2025-10-21T18:00:02.866294Z","shell.execute_reply":"2025-10-21T18:00:11.992923Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeee1ad9eddb4dda85c695f08ff9ecf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72aed74e8f49447fa686c66298891827"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca9e619fb344a70bd6180954ffa868b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d68545a4cdb4ccba41174058a0c262f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fb4cc32eda7446b88fa9c07022248e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3756176a6ae4470ea1dc14e37a59a12c"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Defining configs\nbart_config = {\n    \"max_length\": 472,\n    \"min_length\": 200,\n    \"do_sample\": False,\n    \"length_penalty\": 2.0\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:11.995016Z","iopub.execute_input":"2025-10-21T18:00:11.995281Z","iopub.status.idle":"2025-10-21T18:00:11.999429Z","shell.execute_reply.started":"2025-10-21T18:00:11.995252Z","shell.execute_reply":"2025-10-21T18:00:11.998594Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Trying the Model","metadata":{}},{"cell_type":"code","source":"video_link = \"https://www.youtube.com/watch?v=rHLEWRxRGiM&list=TLPQMjExMDIwMjXbhHt8H4YZWQ&index=3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:12.000149Z","iopub.execute_input":"2025-10-21T18:00:12.000326Z","iopub.status.idle":"2025-10-21T18:00:14.965457Z","shell.execute_reply.started":"2025-10-21T18:00:12.000312Z","shell.execute_reply":"2025-10-21T18:00:14.964685Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"video_id = extract_video_id(video_link)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:14.966272Z","iopub.execute_input":"2025-10-21T18:00:14.966511Z","iopub.status.idle":"2025-10-21T18:00:14.982962Z","shell.execute_reply.started":"2025-10-21T18:00:14.966490Z","shell.execute_reply":"2025-10-21T18:00:14.982329Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"API = YouTubeTranscriptApi()\nfetched_transcript = API.fetch(video_id, languages=['en'])\ntext = \"\\n\".join(snippet.text for snippet in fetched_transcript)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:14.983700Z","iopub.execute_input":"2025-10-21T18:00:14.983953Z","iopub.status.idle":"2025-10-21T18:00:15.980975Z","shell.execute_reply.started":"2025-10-21T18:00:14.983927Z","shell.execute_reply":"2025-10-21T18:00:15.980166Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"response = summarizer(text, **bart_config)\n\nsummary = response[0].get('summary_text')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:15.981800Z","iopub.execute_input":"2025-10-21T18:00:15.982067Z","iopub.status.idle":"2025-10-21T18:00:21.314859Z","shell.execute_reply.started":"2025-10-21T18:00:15.982042Z","shell.execute_reply":"2025-10-21T18:00:21.313978Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print(summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:21.315742Z","iopub.execute_input":"2025-10-21T18:00:21.315975Z","iopub.status.idle":"2025-10-21T18:00:21.321984Z","shell.execute_reply.started":"2025-10-21T18:00:21.315958Z","shell.execute_reply":"2025-10-21T18:00:21.320846Z"}},"outputs":[{"name":"stdout","text":"In the last two videos I talked about linear transformations and matrices. In the next video I'll start getting into the determinant. In general throughout the series we'll work mainly in two dimensions. But once you get all the core ideas in two, they carry over pretty seamlessly to higher dimensions. It turns out that 3D matrix multiplication is actually pretty important for fields  like computer graphics and robotics, since things like rotations and three dimensions are hard to describe but easier to wrap your mind around if you break them down into separate, easier-to-think-about rotations. In fact, a good way to test your understanding of this is to try to reason through what specifically this matrix multiplication should look like, and how it relates to the idea of applying successive transformations in space in 2D. The next video will be about determinant matrix multiplying in 2D and 3D, and the next will be on determinants in 3D and 4D.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Encapsulating Usage in a method","metadata":{}},{"cell_type":"code","source":"def summarize_youtube_video(video_link, summarizer, bart_config):\n    \"\"\"\n    Extracts transcript text from a YouTube video and summarizes it.\n\n    Args:\n        video_link (str): The full YouTube video URL.\n        summarizer (callable): A summarization function or model pipeline (e.g., from transformers).\n        bart_config (dict): Configuration parameters for the summarizer.\n\n    Returns:\n        str: The generated summary text.\n    \"\"\"\n    # Extract video ID\n    video_id = extract_video_id(video_link)\n\n    # Fetch transcript\n    API = YouTubeTranscriptApi()\n    fetched_transcript = API.fetch(video_id, languages=['en'])\n\n    # Combine transcript text\n    text = \"\\n\".join(snippet.text for snippet in fetched_transcript)\n\n    # Generate summary\n    response = summarizer(text, **bart_config)\n    summary = response[0].get('summary_text')\n\n    return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:21.324836Z","iopub.execute_input":"2025-10-21T18:00:21.325105Z","iopub.status.idle":"2025-10-21T18:00:25.129745Z","shell.execute_reply.started":"2025-10-21T18:00:21.325079Z","shell.execute_reply":"2025-10-21T18:00:25.128873Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Try\nprint(summarize_youtube_video(video_link, summarizer, bart_config))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:25.130618Z","iopub.execute_input":"2025-10-21T18:00:25.130931Z","iopub.status.idle":"2025-10-21T18:00:31.133347Z","shell.execute_reply.started":"2025-10-21T18:00:25.130906Z","shell.execute_reply":"2025-10-21T18:00:31.132683Z"}},"outputs":[{"name":"stdout","text":"In the last two videos I talked about linear transformations and matrices. In the next video I'll start getting into the determinant. In general throughout the series we'll work mainly in two dimensions. But once you get all the core ideas in two, they carry over pretty seamlessly to higher dimensions. It turns out that 3D matrix multiplication is actually pretty important for fields  like computer graphics and robotics, since things like rotations and three dimensions are hard to describe but easier to wrap your mind around if you break them down into separate, easier-to-think-about rotations. In fact, a good way to test your understanding of this is to try to reason through what specifically this matrix multiplication should look like, and how it relates to the idea of applying successive transformations in space in 2D. The next video will be about determinant matrix multiplying in 2D and 3D, and the next will be on determinants in 3D and 4D.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Handling Long Videos Issue","metadata":{}},{"cell_type":"markdown","source":"### 1. Defining a function that split long transcript into chunks","metadata":{}},{"cell_type":"code","source":"def chunk_text(text, chunk_size=2000, overlap=100):\n    \"\"\"Splits long text into chunks under the token limit.\"\"\"\n    chunks = []\n    start = 0\n    while start < len(text):\n        end = start + chunk_size\n        chunk = text[start:end]\n        chunks.append(chunk)\n        print(end-start)\n        start += chunk_size - overlap\n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:31.134702Z","iopub.execute_input":"2025-10-21T18:00:31.134929Z","iopub.status.idle":"2025-10-21T18:00:31.139231Z","shell.execute_reply.started":"2025-10-21T18:00:31.134912Z","shell.execute_reply":"2025-10-21T18:00:31.138614Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### 2. New summarization function","metadata":{}},{"cell_type":"code","source":"def summarize_long_youtube_video(video_link, summarizer, bart_config):\n    \"\"\"\n    Fetches YouTube transcript and summarizes it in chunks to handle long videos.\n    \"\"\"\n    # Extract video ID\n    video_id = extract_video_id(video_link)\n\n    # YouTube Transcript API\n    API = YouTubeTranscriptApi()\n    \n    # Fetch transcript\n    fetched_transcript = API.fetch(video_id, languages=['en'])\n\n    # Combine transcript text\n    text = \" \".join([snippet.text for snippet in fetched_transcript])\n\n    # Split into chunks\n    chunks = chunk_text(text)\n\n    # Summarize each chunk\n    partial_summaries = []\n    for chunk in chunks:\n        response = summarizer(chunk, **bart_config)\n        partial_summaries.append(response[0]['summary_text'])\n\n    # Combine and summarize again (hierarchical)\n    combined_summary_text = \" \".join(partial_summaries)\n    if len(combined_summary_text) > 1000:\n        final_summary = summarizer(combined_summary_text, **bart_config)[0]['summary_text']\n    else:\n        final_summary = combined_summary_text\n\n    return final_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:31.139927Z","iopub.execute_input":"2025-10-21T18:00:31.140173Z","iopub.status.idle":"2025-10-21T18:00:31.161928Z","shell.execute_reply.started":"2025-10-21T18:00:31.140153Z","shell.execute_reply":"2025-10-21T18:00:31.161344Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Try\nprint(summarize_long_youtube_video(\"https://www.youtube.com/watch?v=VqrYlDcZQ54\", summarizer, bart_config))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T18:00:31.162542Z","iopub.execute_input":"2025-10-21T18:00:31.162737Z","iopub.status.idle":"2025-10-21T18:01:01.145826Z","shell.execute_reply.started":"2025-10-21T18:00:31.162715Z","shell.execute_reply":"2025-10-21T18:01:01.144563Z"}},"outputs":[{"name":"stderr","text":"Your max_length is set to 472, but your input_length is only 397. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=198)\n","output_type":"stream"},{"name":"stdout","text":"2000\n2000\n2000\n2000\n2000\n2000\n2000\n2000\n2000\n","output_type":"stream"},{"name":"stderr","text":"Your max_length is set to 472, but your input_length is only 431. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=215)\nYour max_length is set to 472, but your input_length is only 427. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=213)\nYour max_length is set to 472, but your input_length is only 422. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=211)\nYour max_length is set to 472, but your input_length is only 410. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=205)\nYour max_length is set to 472, but your input_length is only 399. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=199)\nYour max_length is set to 472, but your input_length is only 418. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=209)\nYour max_length is set to 472, but your input_length is only 415. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=207)\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nYour max_length is set to 472, but your input_length is only 408. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=204)\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [71,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/460155408.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Try\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarize_long_youtube_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.youtube.com/watch?v=VqrYlDcZQ54\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbart_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/1758829904.py\u001b[0m in \u001b[0;36msummarize_long_youtube_video\u001b[0;34m(video_link, summarizer, bart_config)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mcombined_summary_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_summaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_summary_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfinal_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_summary_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbart_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mfinal_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_summary_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         if (\n\u001b[1;32m    188\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m             )\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1336\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2411\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2412\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2413\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0;31m# the manual implementation that requires a 4D causal mask in all cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                 \u001b[0;31m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m                 \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_4d_attention_mask_for_sdpa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m                 \u001b[0;31m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture data-dependent controlflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":19}]}